{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap15 - 오토인코더 - Autoencoder\n",
    "\n",
    "\n",
    "저번 포스팅 [07. 순환 신경망, RNN](http://excelsior-cjh.tistory.com/183)에서는 자연어, 음성신호, 주식과 같은 연속적인 데이터에 적합한 모델인 RNN, LSTM, GRU에 대해 알아보았다. 이번 포스팅에서는 딥러닝에서의 비지도 학습(unsupervised learning)이라고 할 수 있는 **오코인코더**(autoencoder)에 대해 알아보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 오토인코더 란?\n",
    "\n",
    "오토인코더(Autoencoder)는 아래의 그림과 같이 단순히 입력을 출력으로 복사하는 신경망이다. 어떻게 보면 간단한 신경망처럼 보이지만 네트워크에 여러가지 방법으로 제약을 줌으로써 어려운 신경망으로 만든다. 예를들어 아래 그림처럼 hidden layer의 뉴런 수를 input layer(입력층) 보다 작게해서 데이터를 압축(차원을 축소)한다거나, 입력 데이터에 노이즈(noise)를 추가한 후 원본 입력을 복원할 수 있도록 네트워크를 학습시키는 등 다양한 오토인코더가 있다. 이러한 제약들은 오토인코더가 단순히 입력을 바로 출력으로 복사하지 못하도록 방지하며, 데이터를 효율적으로 표현(representation)하는 방법을 학습하도록 제어한다.\n",
    "\n",
    "\n",
    "\n",
    "![](./images/ae01.png)\n",
    "\n",
    "\n",
    "\n",
    "오토인코더는 위의 그림에서 볼 수 있듯이 항상 인코더(encoder)와 디코더(decoder), 두 부분으로 구성되어 있다.\n",
    "\n",
    "- **인코더(encoder)** : 인지 네트워크(recognition network)라고도 하며, 입력을 내부 표현으로 변환한다.\n",
    "- **디코더(decoder)** : 생성 네트워크(generative nework)라고도 하며, 내부 표현을 출력으로 변환한다.\n",
    "\n",
    "오토인코더는 위의 그림에서 처럼, 입력과 출력층의 뉴런 수가 동일하다는 것만 제외하면 일반적인 MLP(Multi-Layer Perceptron)과 동일한 구조이다. 오토인코더는 입력을 재구성하기 때문에 출력을 **재구성(reconstruction)**이라고도 하며, 손실함수는 입력과 재구성(출력)의 차이를 가지고 계산한다. \n",
    "\n",
    "위 그림의 오토인토더는 히든 레이어의 뉴런(노드, 유닛)이 입력층보다 작으므로 입력이 저차원으로 표현되는데, 이러한 오토인코더를 **Undercomplete Autoencoder**라고 한다. undercomplete 오토인코더는 저차원을 가지는 히든 레이어에 의해 입력을 그대로 출력으로 복사할 수 없기 때문에, 출력이 입력과 같은 것을 출력하기 위해 학습해야 한다. 이러한 학습을 통해 undercomplete 오토인코더는 입력 데이터에서 가장 중요한 특성(feature)을 학습하도록 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 일관된 출력을 위해 유사난수 초기화\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# 한글출력\n",
    "# matplotlib.rc('font', family='AppleGothic')  # MacOS\n",
    "matplotlib.rc('font', family='Malgun Gothic')  # Windows\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, shape=[28, 28]):\n",
    "    plt.imshow(image.reshape(shape), cmap=\"Greys\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def plot_multiple_images(images, n_rows, n_cols, pad=2):\n",
    "    images = images - images.min()  # 최소값을 0으로 만들어 패딩이 하얗게 보이도록 합니다.\n",
    "    w,h = images.shape[1:]\n",
    "    image = np.zeros(((w+pad)*n_rows+pad, (h+pad)*n_cols+pad))\n",
    "    for y in range(n_rows):\n",
    "        for x in range(n_cols):\n",
    "            image[(y*(h+pad)+pad):(y*(h+pad)+pad+h),(x*(w+pad)+pad):(x*(w+pad)+pad+w)] = images[y*n_cols+x]\n",
    "    plt.imshow(image, cmap=\"Greys\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Undercomplete Linear 오토인코더로 PCA 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D 데이터셋을 만듦\n",
    "import numpy.random as rnd\n",
    "\n",
    "rnd.seed(4)\n",
    "m = 200\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = rnd.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "data = np.empty((m, 3))\n",
    "data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * rnd.randn(m) / 2\n",
    "data[:, 1] = np.sin(angles) * 0.7 + noise * rnd.randn(m) / 2\n",
    "data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * rnd.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정규화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(data[:100])\n",
    "X_test = scaler.transform(data[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "################\n",
    "# layer params #\n",
    "################\n",
    "n_inputs = 3\n",
    "n_hidden = 2  # coding units\n",
    "n_outputs = n_inputs\n",
    "\n",
    "# autoencoder\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden = tf.layers.dense(X, n_hidden)\n",
    "outputs = tf.layers.dense(hidden, n_outputs)\n",
    "\n",
    "################\n",
    "# Train params #\n",
    "################\n",
    "learning_rate = 0.01\n",
    "n_iterations = 1000\n",
    "pca = hidden\n",
    "\n",
    "# loss\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))  # MSE\n",
    "# optimizer\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(reconstruction_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for iteration in range(n_iterations):\n",
    "        train_op.run(feed_dict={X: X_train})\n",
    "    pca_val = pca.eval(feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(4,3))\n",
    "# plt.plot(pca_val[:,0], pca_val[:, 1], \"b.\")\n",
    "# plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "# plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "# print('pca_val.shape :', pca_val.shape)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/unc-ae.PNG)\n",
    "\n",
    "\n",
    "\n",
    "위의 코드에서 입력의 개수(`n_inputs`)와 출력의 개수(`n_outputs`)가 동일한 것을 알 수 있으며, PCA를 위해 `tf.layers.dense()`에서 따로 활성화 함수를 지정해주지 않아 모든 뉴런이 선형인 것을 알 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stacked 오토인코더\n",
    "\n",
    "**Stacked** 오토인코더 또는 **deep** 오토인코더는 여러개의 히든 레이어를 가지는 오토인코더이며, 레이어를 추가할수록 오토인코더가 더 복잡한 코딩(부호화)을 학습할 수 있다. stacked 오토인코더의 구조는 아래의 그림과 같이 가운데 히든레이어(코딩층)을 기준으로 대칭인 구조를 가진다.\n",
    "\n",
    "![stacked autoencoder](./images/stacked-ae.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 텐서플로로 stacked 오토인코더 구현\n",
    "\n",
    "Stacked 오토인코더는 기본적인 Deep MLP와 비슷하게 구현할 수 있다. 아래의 예제는 [He](http://excelsior-cjh.tistory.com/177?category=940400) 초기화, [ELU](http://excelsior-cjh.tistory.com/178?category=940400) 활성화 함수, $l_2$ 규제(regularization)을 사용해 MNIST 데이터셋에 대한 stacked 오토인코더를 구현한 코드이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 MNIST Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "train_x = train_x.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "test_x = test_x.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "train_y = train_y.astype(np.int32)\n",
    "test_y = test_y.astype(np.int32)\n",
    "valid_x, train_x = train_x[:5000], train_x[5000:]\n",
    "valid_y, train_y = train_y[:5000], train_y[5000:]\n",
    "\n",
    "# Mini-batch\n",
    "def shuffle_batch(features, labels, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(features))\n",
    "    n_batches = len(features) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        batch_x, batch_y = features[batch_idx], labels[batch_idx]\n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "################\n",
    "# layer params #\n",
    "################\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300  # encoder\n",
    "n_hidden2 = 150  # coding units\n",
    "n_hidden3 = n_hidden1  # decoder\n",
    "n_outputs = n_inputs  # reconstruction\n",
    "\n",
    "################\n",
    "# train params #\n",
    "################\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0001\n",
    "n_epochs = 5\n",
    "batch_size = 150\n",
    "n_batches = len(train_x) // batch_size\n",
    "\n",
    "# set the layers using partial\n",
    "he_init = tf.keras.initializers.he_normal()  # He 초기화\n",
    "l2_regularizer = tf.contrib.layers.l2_regularizer(scale=l2_reg)  # L2 규제\n",
    "dense_layer = partial(tf.layers.dense,\n",
    "                      activation=tf.nn.elu,\n",
    "                      kernel_initializer=he_init,\n",
    "                      kernel_regularizer=l2_regularizer)\n",
    "\n",
    "# stacked autoencoder\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "hidden1 = dense_layer(inputs, n_hidden1)\n",
    "hidden2 = dense_layer(hidden1, n_hidden2)\n",
    "hidden3 = dense_layer(hidden2, n_hidden3)\n",
    "outputs = dense_layer(hidden3, n_outputs, activation=None)\n",
    "\n",
    "# loss\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - inputs))\n",
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "# optimizer\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, Train MSE : 0.02203\n",
      "epoch : 1, Train MSE : 0.01116\n",
      "epoch : 2, Train MSE : 0.01044\n",
      "epoch : 3, Train MSE : 0.01036\n",
      "epoch : 4, Train MSE : 0.01054\n",
      "Wall time: 37.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_batches):\n",
    "            batch_x, batch_y = next(shuffle_batch(train_x, train_y, batch_size))\n",
    "            sess.run(train_op, feed_dict={inputs: batch_x})\n",
    "        loss_train = reconstruction_loss.eval(feed_dict={inputs: batch_x})\n",
    "        print('epoch : {}, Train MSE : {:.5f}'.format(epoch, loss_train))\n",
    "    saver.save(sess, './model/stacked_ae.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstructed_digits(X, outputs, model_path=None, n_test_digits=2):\n",
    "    with tf.Session() as sess:\n",
    "        if model_path:\n",
    "            saver.restore(sess, model_path)\n",
    "        outputs_val = outputs.eval(feed_dict={inputs: test_x[:n_test_digits]})\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    for digit_index in range(n_test_digits):\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 1)\n",
    "        plot_image(test_x[digit_index])\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 2)\n",
    "        plot_image(outputs_val[digit_index])\n",
    "        \n",
    "# show_reconstructed_digits(inputs, outputs, './model/stacked_ae.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드를 학습 시킨 후에 테스트셋의 일부를 재구성하였을 때, 아래의 그림과 같은 결과가 나온다.\n",
    "\n",
    "![](./images/stacked-ae02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 가중치 묶기\n",
    "\n",
    "위(3.1)에서 구현한 stacked 오토인코더처럼, 오토인코더가 완전히 대칭일 때에는 일반적으로 인코더(encoder)의 가중치와 디코더(decoder)의 가중치를 묶어준다. 이렇게 가중치를 묶어주게 되면, 네트워크의 가중치 수가 절반으로 줄어들기 때문에 학습 속도를 높이고 오버피팅의 위험을 줄여준다.\n",
    "\n",
    "![](./images/stacked-ae03.PNG)\n",
    "\n",
    "\n",
    "\n",
    "위의 그림을 수식으로 나타내면, 예를들어 오토인코더가 $N$개의 층을 가지고 있고 $\\mathbf{W}_L$ 이 $L$ 번째 층의 가중치를 나타낸다고 할 때, 디코더 층의 가중치는 $\\mathbf{W}_{N-L+1} = \\mathbf{W}_{L}^{T}$로 정의할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "################\n",
    "# layer params #\n",
    "################\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300  # encoder\n",
    "n_hidden2 = 150  # coding units\n",
    "n_hidden3 = n_hidden1  # decoder\n",
    "n_outputs = n_inputs  # reconstruction\n",
    "\n",
    "################\n",
    "# train params #\n",
    "################\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0005\n",
    "n_epochs = 5\n",
    "batch_size = 150\n",
    "n_batches = len(train_x) // batch_size\n",
    "\n",
    "# set the layers using partial\n",
    "activation = tf.nn.elu\n",
    "weight_initializer = tf.keras.initializers.he_normal()  # He 초기화\n",
    "l2_regularizer = tf.contrib.layers.l2_regularizer(scale=l2_reg)  # L2 규제\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "W1_init = weight_initializer([n_inputs, n_hidden1])\n",
    "W2_init = weight_initializer([n_hidden1, n_hidden2])\n",
    "\n",
    "# Encoder weights\n",
    "W1 = tf.Variable(W1_init, dtype=tf.float32, name='W1')\n",
    "W2 = tf.Variable(W2_init, dtype=tf.float32, name='W2')\n",
    "# Decoder weights\n",
    "W3 = tf.transpose(W2, name='W3')  # 가중치 묶기\n",
    "W4 = tf.transpose(W1, name='W4')  # 가중치 묶기\n",
    "\n",
    "# bias\n",
    "b1 = tf.Variable(tf.zeros(n_hidden1), name='b1')\n",
    "b2 = tf.Variable(tf.zeros(n_hidden2), name='b2')\n",
    "b3 = tf.Variable(tf.zeros(n_hidden3), name='b3')\n",
    "b4 = tf.Variable(tf.zeros(n_outputs), name='b4')\n",
    "\n",
    "hidden1 = activation(tf.matmul(inputs, W1) + b1)\n",
    "hidden2 = activation(tf.matmul(hidden1, W2) + b2)\n",
    "hidden3 = activation(tf.matmul(hidden2, W3) + b3)\n",
    "outputs = tf.matmul(hidden3, W4) + b4\n",
    "\n",
    "# loss\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - inputs))\n",
    "reg_loss = l2_regularizer(W1) + l2_regularizer(W2)\n",
    "loss = reconstruction_loss + reg_loss\n",
    "\n",
    "# optimizer\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# saver\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, Train MSE : 0.01495\n",
      "epoch : 1, Train MSE : 0.01514\n",
      "epoch : 2, Train MSE : 0.01716\n",
      "epoch : 3, Train MSE : 0.01761\n",
      "epoch : 4, Train MSE : 0.01720\n",
      "Wall time: 41.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_batches):\n",
    "            batch_x, batch_y = next(shuffle_batch(train_x, train_y, batch_size))\n",
    "            sess.run(train_op, feed_dict={inputs: batch_x})\n",
    "        loss_train = reconstruction_loss.eval(feed_dict={inputs: batch_x})\n",
    "        print('epoch : {}, Train MSE : {:.5f}'.format(epoch, loss_train))\n",
    "    saver.save(sess, './model/stacked_ae_tying.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/stacked_ae_tying.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbkAAAD+CAYAAACnfntIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGA1JREFUeJzt3Vms3dP7x/FVLZ2Otk51cGhVnU5qahFVJNR0ISGIC3pBkGgkJJoglUhwwx29EYSYLyQEIVpBSEsMVWqs01m1R3s6T6eT4X/jv36f59G97LOdfdo+5/26elaevfd3nybdK9/n+a61evz1118JAICIjjjYXwAAgHphkgMAhMUkBwAIi0kOABAWkxwAICwmOQBAWExyAICwmOQAAGExyQEAwurVxddje5VDR4+D/QWAw01rayu/YYeIpqamqn7DuJMDAITFJAcACItJDgAQVlf35AAABXoyTI8eXd8612v66//5558V33ewv3cl3MkBAMJikgMAhEW5EgAOIn9wdbXlwj/++KP4OeqII/53P3P00Ueb3L59+6q+vn6Ov/6ePXty3LNnz4rfpatxJwcACItJDgAQFpMcACAsenIAcBD5vle1PTn/OL/25Pr06VP1+3r1stPA/v37K15f+279+vUzuSOPPLLiNSp9zwNdo7NxJwcACItJDgAQFuVKAKgDLcv5Ep2O/eP2Wr7T0qHP+TKffo5fFtDQ0FDxen4pgL7Xlx31mps2bTK5o446KsdaujzQNZUuS/B/b2fgTg4AEBaTHAAgLCY5AEBY9OQAoA5Ku/JrH6zU9/K9PO1f+dzvv/+eY78sQHPaO0vpn/0zHft+nX5X/z59rX9f6Xql1+r19G/vCO7kAABhMckBAMKiXAkAnaDWnTz84/5aWty+fbvJtbe359iX73QHEl0ykFJKAwcOzLEvD5ZKkr60WToFQfnvpmO/nED//tJSi1pxJwcACItJDgAQFpMcACCsw74n9/nnn5vx7Nmzc3z88cebXN++fXN80003mVxjY+MBYwCoRqmf5JcJ6PZVO3fuNLk1a9bk+LfffjO5zZs351hPBEjJLhtoamoyuf79+1d8n36m/5zjjjvO5ErbiumJ4/762iMs9fm8zjihgDs5AEBYTHIAgLB6dMYjmh3Q6RcbN26cGS9durSmz9FHbKdMmfKfvlMtRo0aleNZs2aZ3MiRI+txyfqeVAgE1Nraan7DSiVJzemygJRS2rJlS45//fVXk/vxxx9z7MuVeg39jJRsCdSXBPVx/9WrV5vc+vXrzbh379459uVKPYzV71wyYsSIHF966aUmN3HixBz7w1ZL5crS6QVNTU1V/YZxJwcACItJDgAQFpMcACCsw34JwZtvvmnGixYtyrHWgVOyte4vvvjC5N56660cv/feeyZ30kkn5XjlypVVfze/E7jWt30dXml/LqWU7rvvvqqvCaDr6CPuvn+kPTJ/4rUuG9i2bVvFnPbHUkppx44dOd66dWvF7+IfvdfX+iULurQqpZQGDx6cY10WkFJKa9euzfGyZctMTn9v/bMSzc3NOfZbjmn/0PcuS/26anEnBwAIi0kOABDWYV+unDBhQnGsTj/99BzfcMMNJvfoo4/meNWqVSan5coVK1ZU/d38Y7xartTPTCmlDRs25Hj8+PFVXwPAocEvx9JSmy8Raitj7969Jjds2LAc+8ft29racnzMMceYnO70ryXHlMrLC/x4wIABOdbyaEopzZ07N8cLFy40OV3S4E9WKB0Eq2VeX65kxxMAAAqY5AAAYTHJAQDCOux7cp1Ft6sp9cRKPb9/o8sWNm7caHLnnntuji+//PKarwHg4PC9Jt1KS08BSMn22vzvjfah/OP92rPyJ3PrZ+o2hSn9s0em/HfT7bqWLFlicu+8806O/ZKJ4cOH53jQoEEm55dCKP2b6rHNJHdyAICwmOQAAGFRrqyjXbt2mfE111yTY7+S//HHH8+xL1EAOPT5x921ROcf09eynN/pX5cX+LKjLkXwZcbS74YelOrLnH5XEy0f+uVUra2tFd83efLkHJ988skm508sULt3786xL4HqsohacScHAAiLSQ4AEBaTHAAgLHpydfT888+b8bp163Lst9058cQTu+IrAaiTjmxBpb0m31vTsX+kXpc6+VNOtJemPbiUbB9OP8O/LyV7Qsrnn39ucrrN1znnnGNyum3i0KFDTU57bb7PVuq76d9f6xZf3MkBAMJikgMAhEW5spMtX748xzNnzqz4us8++8yMdbcAAIcfX1rUsX80XnO+XKnlu9LJBps3bzY5fRTfn2ygu6H45Ut+qdOcOXNy/Mknn6RKJk2aZMa6G5QvV6pSubJ0ekGtuJMDAITFJAcACItJDgAQFj25Tvb222/nWE/jTSml66+/PsejR4/usu8EoP58/0i38vJbaek2V77vpr8bvkel423btpmc9uQaGhpMTrfg8t/Tn4gyb968HH///fcmN3Xq1BzrySkppTRixIgc+xPNdUmD78mVcp2BOzkAQFhMcgCAsJjkAABh0ZP7j3zf7Y033sixPw33kUceybFfNwMgLr91lv7/91tw6Ro336PSz/FH6zQ2Nh4wTsn2B/W4npRS+u6778z4hx9+qPg5epyOrotLyfYB/ZZj2nf0fUbN+ff5NX214E4OABAWkxwAICzKlf/Rs88+a8bz58/P8Y033mhyLBsAuiffulC+JKfLDfzyAi1z+tO2dZmAP2lAS4Q//fSTyX300UdmrEsRpk2bZnKXXXZZjocNG2ZyujShvb3d5LZv355jX5LUv8OXdfUzOYUAAACHSQ4AEBaTHAAgLHpyHbRo0SIzvvPOO8140KBBOX744Ye75DsBOPRor620rZdfJlB63F6XAvj3ac/KH5+jR4C9+OKLJvfFF1+Y8ahRo3J8xRVXmJwuG/A9Mv2u/higavtpvj+pfyM9OQAAHCY5AEBYlCuroI/U3nDDDSbnyxDTp0/PMUsGgO7Dl9p07JcC6G9Kqezod/PXpQj+UXz9LWprazM53Ynp9ddfr/iZKaV0ySWX5Pjss882uYEDB+a49Dft2LHD5HRJg9/tSf/+0qnhteJODgAQFpMcACAsJjkAQFj05A7A19avvPLKHLe0tJic34n7oYceqt8XA3DI8v0j//i/8ltyVdK/f/+K7/PX27JlS479id562re+LqWULrjgAjM+//zzc6ynffvr+5PJdesuPUkhJfub6rcc01ytywRKuJMDAITFJAcACIty5QH41foff/xxxde+9NJLZuwPGQQQl5YM/XKi0m4d+vh/qazpS5L6+L1/39atW3O8ePFik9MdT3RXppRSuvDCC814/PjxOfblUr2m/5v0EFe/vKH0b6Hjzlgy4HEnBwAIi0kOABAWkxwAICx6cn/Tx2GnTJlS8XUvv/yyGU+aNKlu3wnAoa30+Ltul6VbXqWU0p49e3Lc0NBgctrb8+/T6/mcLm9asGCByek2XxMnTjQ5/3unPTv/uL/SExE831vzJ34r3R6sdApBrbiTAwCExSQHAAiLcuXfnnvuuRyvWLGi4uv87gD1WKEP4PCnZbjSDidaukzJPn6/c+dOk9Pd/f1JA1qi/PHHH01Or69LBFJKafjw4RVf65dF6BICX1r0pxIoLXv6z9TPYQkBAAAdwCQHAAiLSQ4AEFa37cktXbrUjB988MGD80UAhFd6pN739bVH53ty2odbsmRJxdyQIUNMbvDgwTlubm42uf3795txe3v7Ab+n57fuKi2nKJ3+XW/cyQEAwmKSAwCE1W3LlfPnzzdjPfDP04NRdadtAKhGaTcU/yi+lgj9Y/l6CoEvF+oOJGPGjDG5Y489NsdDhw41OV+u1FNY9H3++v67aRnS/03Kv6/ey7C4kwMAhMUkBwAIi0kOABBWt+3JlUydOtWM33///RzTkwPwX/mtrZT21vwp3vr7069fP5PTZQN+GzHtAfqcP/1be2t+yzHle2mlJQSl99Ubd3IAgLCY5AAAYfUo7RxdB116MRRxfALQQa2trYfMb1hHTgHQ3L/95pd2JzmUTl1pamqq6stwJwcACItJDgAQFpMcACCsru7JAQDQZbiTAwCExSQHAAiLSQ4AEBaTHAAgLCY5AEBYTHIAgLCY5AAAYTHJAQDCYpIDAITFJAcACItJDgAQFpMcACAsJjkAQFhMcgCAsJjkAABhMckBAMJikgMAhMUkBwAIi0kOABBWry6+3l9dfD1U1uNgfwHgcNPS0sJv2CFi3LhxVf2GcScHAAiLSQ4AEBaTHAAgrK7uyQFAt9ejx//aSX/9Zdt8OtbXlT6jIzl/vY68988//6z4OUcccUTF3MHEnRwAICwmOQBAWJQrAaCL1VrO0/f9/vvvFXM9e/Y0OR337t3b5Hbv3m3G+/fvz/G+fftMTt/bq5edPtrb2yteo1QCrXeZkzs5AEBYTHIAgLCY5AAAYXXbntwrr7xixrt27crxwoULTe7pp5+u+DkPPPCAGU+bNi3HF1100X/4hgC6g9ISAp/bs2dPjnfu3Fnxffqof0q2z+b7Y/369TNj7a3p9VKyvb2RI0eanPbo/vjjj4rv8/S7an8upc7p0XEnBwAIi0kOABBWjy5emX5Ql8HfcccdOX7qqafqco1TTjklx5988onJDRw4sC7XrBGnEAAdVOspBKXfWV8S1KUBbW1tJrdly5Ycr1ixwuS2bduW47Vr15rckUcemWP/eL9fJjBgwIAcjx071uSam5tz7JcQDBo0KMdHH320yR111FE59qVU/Rz/b1FaesApBACAbo9JDgAQFpMcACCs0EsItAeXUvV9uEmTJpnxddddl+OlS5ea3AsvvGDGP/30U45fe+01k7v11luruj6Aw1/pNAHdSmv79u0mp3241tbWirmWlhaTW716dY59n037XtofS+mfPbLRo0fneO/evSanfbfGxkaT036aLkPw1/DLCfS1pe3ISv25Eu7kAABhMckBAMIKV67UW/Znnnmm4uvOOeccM547d26O/Q4AenvvV/IvW7bMjD/99NMcb9y4sYpvDCAiLa+VdiDxpb0NGzbk2C8F0N8UX76bMmVKjk888UST098tXwJdt26dGesSBr8DiZY9/TIBXSK1detWk9PyqT89Qf9tfCm1M3AnBwAIi0kOABAWkxwAIKxwPTmtWfutdLQP98EHH5hcQ0NDVZ///PPPm/GCBQsqvvbqq6+u6jMBHJ60L1Y6TcA/il9aXqDbbvXt29fkdOf/iRMnmtwZZ5yRY7+FoPb23n33XZPzy6I2bdqUY9/b0+UOupwgJfu4v/+30C3H+vfvb3Lan9Q4Jft8hN9GrFrcyQEAwmKSAwCEFa5cOXny5Bz7R/j18VRfBqiWX5bgdxYA0D35Ep3uauLbIX369MmxX5akhgwZYsbDhw/P8cknn2xyugOJP1C1dHrB999/b8Za6vTlQx3rQdMp2aUAfpmAnoLg36ev9csSat3lRHEnBwAIi0kOABAWkxwAIKxwPTnVWSdxv/TSSzn+9ttvi6+9/PLLc+xr5gDi8jvoa3/Jb4+lvSb/SL32r3xPbOjQoRXfp6cA/PDDDyb35ptv5njhwoUmp/3BlOz2YFOnTjU5/U3zvUT9rv6ZB32t38bsmGOOybH/d/LjWnAnBwAIi0kOABBW6HJlrb755hszvv3223Psdy447rjjzHj27Nk51rIDgNj8SQNaavOPwuvuHf59uhRh8ODBJqclSl8u1F1N/I5OX375ZY59CfTMM88044svvjjHZ599tsnpUgh/eoF+rv/tK+1comXe0r+hz1WLOzkAQFhMcgCAsJjkAABh0ZM7gM8++8yMfR9OzZgxw4zHjh1bl+8E4NBWetzdb3OlPSr/Pu3J+f6VjnVn/5RS+u6773K8aNEik/vtt99y7E8vGDdunBnrb5jfZkv/Dr+NmS5F8D1I7bv5nPby/L+F9h1r3eKLOzkAQFhMcgCAsChX/u2WW27J8auvvlrxdXfffbcZ33vvvXX7TgAObb5kp7T05suOWobzOX38XsuaKaW0YcOGHK9atcrkdJnAL7/8YnIjRozIse6akpLdccRf058YoK0bf9LBsccem+NSadGf3KL/Fn73lc7AnRwAICwmOQBAWExyAICwum1PzteT58yZk2PdzTullIYNG5bj+++/3+T0tHEA+H/ar/NbUunYn16gvzd+Cy5dCuC3H2xtbc3xqFGjTG7QoEE59lsRnnbaaWZcWqagJ4z7vpsuL/Dbeulvqs9pj84vtdDf11L/s4Q7OQBAWExyAICwum258vrrrzfjtra2iq+96667ctzY2Fi37wTg8OXLaVr28zkt9flH+PV9mzZtMrk1a9bk+OeffzY5XTbgf6d69+6d4xNOOMHk/K4mWiL15VJ93N+XHXVcKld6WpIs7f5SK+7kAABhMckBAMJikgMAhNWtenILFy7M8ccff1zxdddee60Zz5w5s15fCUAQpd31/dZdun1VaRnSjh07zHjp0qU59icN7N69O8cjR440uebm5hyfcsopJqfbcaVkH+P3W3Dp36GnlKdk/37/9+p2YD6ny7l8f9Avr6gFd3IAgLCY5AAAYTHJAQDCCt2T0xp1SinNmjUrx77WrM466ywzZusuAP+mtKbL95a0J+dzW7duzfHKlStNTsfbt283Oe2tjRkzxuQmT56cY79Ozv++aY+sb9++JqfjhoYGk9M1dH59n24H5tfQaS/P/1tojm29AABwmOQAAGGFLlc++eSTZvzhhx9WfK2eDM6SAQDVKJ2ArXxJUE8h8FtnaWnPb921YsWKHPvS3oQJE3J86qmnmpyeBu7Lfv6kgdLj/vqIv2/5aCl1/fr1JqelzOOPP97kBg8efMDXedX+W3vcyQEAwmKSAwCExSQHAAgrdE/On+Jd8thjj+WYJQMAOqrUM/Ing2s/S3tgKaW0bNmyA8Yp2dO/hwwZYnJNTU0Vc8r3y7QHmFJKu3btyrE/RVyPwlm+fLnJffXVVzn2yxvGjh2bY//7qv8WvgeofUd6cgAAOExyAICwQpcrO0JX+fvTaTtCT+D1j/jq47G+RKH8Ti2zZ8+u6tr+elqu9bsMAOhc/vF3Lcv5/5v6f3zz5s0mt27duhz7Uwh0uYE/bbu9vT3HelpBSimtXbs2x7486Xcn0bKgLzvqCQWrV682OV3u4H9vdDcW/9tX7/YQd3IAgLCY5AAAYTHJAQDCoif3N7/VTK1mzJiRY32kNyVba3/iiSc65Xol+jfddtttdb8e0N3oFlmlU6z9EoLSKQTaayudlO17cl9//XWO16xZU/H6fjsu3yPT/plfwqB9R+3zpWR7a7qNWEr27/XLBEr/hrUuG1DcyQEAwmKSAwCEFbpcOX36dDN+7rnn6n5Nf/JBtfQWvlT2uPnmm834vPPOq/ja888/v6bvAqA6pXKaPm7vH6nXEp2+LqWUBg4cmOMRI0aYnB5+unjxYpNraWnJ8bx580xOl0Xp56eU0oABA8xYly34g1H1u/oDVXV3lFGjRpncSSedlGO/ZKBfv345Lh2oWivu5AAAYTHJAQDCYpIDAITVw58SW2ddejHvxRdfzLF/jLbk22+/zXFHHv2/5557zLi5ubnia6+66qoc+8dv6+S/F7uBbqalpaXq3zDtL/nTv0u9Jt1isK2tzeT0FIJVq1b575Zj36/TZQL9+/c3udJJ3b5HpksB/EkHjY2NOR45cqTJaW9PPz8l+wyCX15Q+ncaN25cVb9h3MkBAMJikgMAhNWtypUwKFcCHdSRcqWW2nzZrbRMSEuC/mQDfZ+eOpCSPc1Ad1dKyR6U6j/TLynQR/r9a3Upgt+NpbQUQF/r5xxdluD/XUonwlCuBAB0e0xyAICwmOQAAGHRk+u+6MkBHdSRnlypn1Qt3/fq3bv3AWN/PX/qgS6Z8r/5fqynG/hTw3VLLv++0vVLPUjt+3VkGy96cgCAbo9JDgAQVuhTCADgYNGSnS/D6diX9jTnD0bVnVN27dplcjr2j/ArXaJwoNfq9/EnFCj/3fR7+1KtX4pQ6X31wJ0cACAsJjkAQFhMcgCAsOjJAUCd+cftfR+u0mt936vaZQKebp1V6gH6sf/M0lZl1fYguxp3cgCAsJjkAABhUa4EgC5WKt9pudK/rlSu9EsDlO44oqVL/5m1fs+O5Load3IAgLCY5AAAYTHJAQDC6upTCAAA6DLcyQEAwmKSAwCExSQHAAiLSQ4AEBaTHAAgLCY5AEBYTHIAgLCY5AAAYTHJAQDCYpIDAITFJAcACItJDgAQFpMcACAsJjkAQFhMcgCAsJjkAABhMckBAMJikgMAhMUkBwAIi0kOABAWkxwAICwmOQBAWExyAICwmOQAAGExyQEAwvo/nQ4qaJIwJ1wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_reconstructed_digits(inputs, outputs, './model/stacked_ae_tying.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
