# 5. 심층 신경망 훈련



저번 포스팅 [04. 인공신경망](http://excelsior-cjh.tistory.com/172?category=940400)에서 예제로 살펴본 신경망은 hidden layer가 2개인 얕은 DNN에 대해 다루었다. 하지만, 모델이 복잡해질수록 hidden layer의 개수가 많은 신경망 모델을 학습시켜야 한다. 이러한 깊은 DNN 모델을 학습시키는데에는 다음과 같은 문제가 발생할 확률이 높다.

- **그래디언트 소실**(vanishing gradient) 또는 **폭주**(exploding)가 발생할 수 있다.
- 모델이 복잡하고 커질수록 **학습시간이 매우 느려진다**.
- 모델이 복잡할수록 **오버피팅(overfitting)**될 위험이 크다.

이번 포스팅에서는  이러한 문제들에 대해 알아보고 해결할 수 있는 방법에 대해  알아보도록 하자.



## 1. 그래디언트 소실과 폭주 문제

[03. 오차역전파](http://excelsior-cjh.tistory.com/171?category=940400)에서 살펴보았듯이 역전파 알고리즘은 출력층(output layer)에서 입력층(input layer)로 오차 그래디언트(gradient)를 흘려 보내면서 각 뉴런의 입력값에 대한 손실함수의 그래디언트를 계산한다. 이렇게 계산된 그래디언트를 경사 하강법(gradient descent)단계에서 각 가중치 매개변수($\mathbf{W}$)를 업데이트 해준다. 

하지만, 아래의 그림과 같이 깊이가 깊은 심층신경망에서는 역전파 알고리즘이 입력층으로 전달됨에 따라 그래디언트가 점점 작아져 결국 가중치 매개변수가 업데이트 되지 않는 경우가 발생하게 된다. 이러한 문제를 **그래디언트 소실**(vanishing gradient)라고 한다. 

![](./images/vanishing.png)



그래디언트 소실과는 반대로 역전파에서 그래디언트가 점점 커져 입력층으로 갈수록 가중치 매개변수가 기하급수적으로 커지게 되는 경우가 있는데 이를 **그래디언트 폭주**(exploding gradient)라고 하며, 이 경우에는 발산(diverse)하게되어 학습이 제대로 이루어지지 않는다. 



## 2. 활성화 함수

[04. 인공신경망 - 2.4.1](http://excelsior-cjh.tistory.com/172?category=940400)에서 활성화 여러 종류의 활성화 함수에 대해 알아 보았듯이, 아래의 그림(출처: cs231n)에서 함수 $f$와 같이 입력 신호의 총합($\sum_{i}^{}{w_ix_i + b}$)을 출력 신호로 변환하는 함수를 **활성화 함수**(activation function)라고 한다.

![](./images/activations.png)





### 2.1 시그모이드 함수

시그모이드 함수($\sigma$, sigmoid)는 대표적인 활성화 함수라고 할 수 있으며, 아래와 같은 식을 가지는 함수이다.
$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$
시그모이드 함수는 다음과 같은 특성을 가진다.

- 입력 신호의 총합을 0에서 1사이의 값으로 바꿔준다.
- 입력 신호의 값이 커질수록(작아질수록) 뉴런의 활성화률(firing rate)이 $1$(작아질 경우 $0$)로 수렴(saturation)한다.

![](./images/sigmoid2.png)



하지만, 위와 같은 특성 때문에 시그모이드 함수는 2가지 문제가 있다.

- 입력의 절대값이 크게 되면 0이나 1로 수렴하게 되는데, 이러한 뉴런들은 **그래디언트를 소멸(kill) 시켜 버린다**. 그 이유는 수렴된 뉴런의 그래디언트 값은 0이기 때문에 역전파에서 0이 곱해지기 때문이다. 따라서, 역전파가 진행됨에 따라 아래 층(layer)에는 아무것도 전달되지 않는다.(시그모이드의 도함수는 $\sigma(1- \sigma)$이므로 함수의 값이 0이나 1에 가까우면 도함수의 결과가 매우 작아진다.)
- **원점 중심이 아니다(Not zero-centered)**.  따라서, 평균이 $0$이 아니라 $0.5$이며, 시그모이드 함수는 항상 양수를 출력하기 때문에 출력의 가중치 합이 입력의 가중치 합보다 커질 가능성이 높다. 이것을 편향 이동(bias shift)이라 하며, 이러한 이유로 **각 레이어를 지날 때마다 분산이 계속 커져** 가장 높은 레이어에서는 활성화 함수의 출력이 0이나 1로 수렴하게 되어 그래디언트 소실 문제가 일어나게 된다.



### 2.2 하이퍼볼릭 탄젠트 함수(tanh)

하이퍼볼릭 탄젠트 함수(tanh, hyperbolic tangent)는 시그모이드 함수의 대체제로 사용할 수 있는 활성화 함수이며 아래와 같은 식을 갖는 함수이다.
$$
\begin{align*}
\text{tanh}(x) &= \frac{1-e^{-x}}{1+e^{-x}} \\ &= \frac{2}{1+e^{-2x}} -1
\end{align*}
$$
tanh함수는 시그모이드 함수($\sigma$)와 유사하며, 아래와 같이 시그모이드 함수를 이용해 tanh 함수를 나타낼 수 있다.

$$
\text{tanh}(x) = 2 \sigma(2x)-1
$$
tanh함수는 아래의 그림과 같이 입력값의 총합을 -1에서 1사이의 값으로 변환해 주며, 원점 중심(zero-centered)이기 때문에, 시그모이드와 달리 편향 이동이 일어나지 않는다. 하지만, tanh함수 또한 입력의 절대값이 클 경우 -1이나 1로 수렴하게 되므로 그래디언트를 소멸시켜 버리는 문제가 있다. 

![](./images/tanh.png)



### 2.3 ReLU (Rectified Linear Unit)

ReLU(렐루, Rectified Linear Unit)는 시그모이드 계열과는 다른 활성화 함수이며,  아래의 식과 같이 입력이 0이상이면 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수이다.
$$
\text{ReLU}(x) = \max(0, x)
$$
ReLU함수는 다음과 같은 특성을 가진다.

- 0 이상인 곳에서는 수렴하는 구간이 없다.
- 단순히 입력값을 그대로 출력으로 내보내기 때문에 시그모이드 함수에 비해 계산 속도가 빠르다.
- sigmoid/tanh에 비해 stochastic gradient descent(SGD)에서 수렴속도가 무려 6배나 빠르다고 한다([Krizhevsky et al.](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)).

![](./images/relu.PNG)





## 3. 수렴하지 않는 활성화 함수

2.3 에서 살펴본 ReLU함수 또한 완벽하지 않다. ReLU의 문제는 모델이 학습하는 동안 **일부 뉴런이 0만을 출력하여 활성화 되지 않는 문제**인데, 이러한 문제를 **dead ReLU**라고 한다. 특히 학습률(learning rate)이 클 경우, 모델의 뉴런이 절반정도가 죽어 있기도(뉴런이 0만 출력) 한다. 이렇게 뉴런이 0만을 출력하는 이유는 학습이 진행되면서 뉴런의 **가중치가 업데이트 되면서 가중치 합이 음수가 되는 순간** ReLU에 의해 그 이후로는 0만 출력하게 되며, 이때의 그래디언트 값이 0이 되기 때문이다.

이러한 **dead ReLU**문제를 해결하기 위해 ReLU함수를 조금씩 변형시켜 다양한 ReLU Family를 만들어 사용하기도 한다. 대표적인 ReLU Family를 알아보도록 하자.



###  3.1 LeakyReLU & PReLU

LeakyReLU는 다음과 같은 식을 가지는 ReLU의 변형된 함수이다.
$$
\text{LeakyReLU}_{\alpha}(x) = \max(\alpha x, x)
$$
위의 식에서 하이퍼파라미터인 $\alpha$가 LeakyReLU함수의 새는(leaky, 기울기)정도를 결정하며, 일반적으로 $\alpha = 0.01$로 설정한다. 즉, 0 이하인 입력에 대해 활성화 함수가 0만을 출력하기 보다는 입력값에 $\alpha$만큼 곱해진 값을 출력으로 내보내어 dead ReLU문제를 해결한다.

![](./images/leaky.PNG)



PReLU(Parametric ReLU)는 Leaky ReLU와 식이 동일하지만, LeakyReLU에서 하이퍼파라미터인 $\alpha$를 가중치 매개변수와 마찬가지로 $\alpha$의 값도 학습되도록 역전파에 의해 $\alpha$의 값이 변경되는 함수이다. PReLU는 대규모 이미지 데이터셋에서는 ReLU보다 성능이 좋았지만, 소규모 데이터셋에는 오버피팅될 위험이 있다.



### 3.2 ELU (Exponential Linear Unit)

ELU(Exponential Linear Unit)은 2105년 [B.Xu et al.](https://arxiv.org/pdf/1507289v5.pdf)의 논문에서 제안된 활성화 함수이다. 
$$
\text{ELU}_{\alpha} = \begin{cases} \alpha \left( \exp{(x)} - 1 \right) & \text{if }x <0 \\ x & \text{if } x \ge 0 \end{cases}
$$


![](./images/elu.PNG)



ELU는 ReLU와 달리 다음과 같은 특성을 가진다.

- $x < 0$일 때 ELU 활성화 함수 출력의 평균이 0(zero mean)에 가까워지기 때문에 편향 이동(bias shift)이 감소하여 그래디언트 소실 문제를 줄여준다. 하이퍼파라미터인 $\alpha$는 $x$가 음수일 때 ELU가 수렴할 값을 정의하며 보통 1로 설정한다.
- $x < 0$ 이어도 그래디언티가 0이 아니므로 죽은(dead) 뉴런을 만들지 않는다. 
- $\alpha = 1$일 때 ELU는 $x=0$에서 급격하게 변하지 않고 모든 구간에서 매끄럽게 변하기 때문에 경사하강법에서 수렴속도가 빠르다.



> **TLDR** : *어떠한 활성화 함수를 써야할까?*
>
> 일반적으로 ELU → LeakyReLU → ReLU → tanh → sigmoid 순으로 사용한다고 한다. cs231n 강의에서는 ReLU를 먼저 쓰고 , 그다음으로 LeakyReLU나 ELU 같은 ReLU Family를 쓰며, sigmoid는 사용하지 말라고 하고 있다.



## 4. 가중치 초기화 (Weight Initialization)

신경망 학습에서 중요한것 중 하나는 학습 시킬 때의 가중치 초기값이다. 가중치 초기값을 어떻게 초기화 하느냐에 따라 학습이 잘될 때가 있고 잘 안될 때가 있다.  이번에는 가중치를 초기화하는 방법에 대해 알아보도록 하자.



### 4.1 가중치 초기값이 0이거나 동일한 경우

가중치의 초기값을 모두 0으로 초기화하거나 동일한 값으로 초기화할 경우 모든 뉴런의 동일한 출력값을 내보낼 것이다.  그렇게 되면 역전파(backpropaation) 단계에서 각 뉴런이 모두 동일한 그래디언트 값을 가지게 된다. 학습이 잘 되려면, 각 뉴런이 가중치에 따라 비대칭(asymmetry, 어떤 뉴런은 가중치가 크고 어떤 뉴런은 가중치가 작게 되게끔)이어야 하는데, 모든 뉴런이 동일한 그래디언트로 가중치 값이 변경되므로 뉴런의 개수가 아무리 많아도 뉴런이 하나뿐인 것처럼 작동하기 때문에 학습이 제대로 이루어지지 않는다.  따라서 가중치 초기값을 동일한 값으로 초기화 해서는 안된다.



### 4.2 작은 난수 (Small Random numbers) 인 경우 

가중치 초기값은 작은 값으로 초기화 해야하는 데, 그 이유는 활성화 함수가 sigmoid일 경우 만약 가중치 초기값(절대값)을 큰 값으로 한다면 0과 1로 수렴하기 때문에 그래디언트 소실이 발생하게 된다. 또한 활성화 함수가 ReLU일 경우 절대값이 클 경우 음수일 때는 dead ReLU 문제가 발생하고, 양수일 때는 그래디언트 폭주가 일어나게 된다. 

따라서, 가중치 초기값을 작게 초기화 해야하며 동일한 초기값을 가지지 않도록 랜덤하게 초기화 해야한다. 

[1.1 - 활성화 함수]에서 시그모이드 함수는 그래디언트 소실 문제가 있다는 것을 알았다. 